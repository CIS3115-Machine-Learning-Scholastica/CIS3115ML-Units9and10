{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcP0RHTjWP1q"
   },
   "source": [
    "# Practice 10: Kaggle's CIFAR10 with CNNs\n",
    "\n",
    "Use this notebook as the starting point for the Practice activities.\n",
    "\n",
    "Student Name:    **[  Put your Name Here ]**\n",
    "\n",
    "\n",
    " [Video Walkthough of Practice9.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmZ4LSJ7XLR9"
   },
   "source": [
    "# Section 0\n",
    "\n",
    "=== *You must run this section to set up things for any of the sections below * ===\n",
    "### Setting up Python tools\n",
    "\n",
    "\n",
    "\n",
    "We'll use three libraries for this tutorial: \n",
    "- [pandas](http://pandas.pydata.org/) : dataframes for spreadsheet-like data analysis, reading CSV files, time series\n",
    "- [numpy](http://www.numpy.org/) : for multidimensional data and linear algebra tools\n",
    "- [matplotlib](http://matplotlib.org/) : Simple plotting and graphing\n",
    "- [seaborn](http://stanford.edu/~mwaskom/software/seaborn/) : more advanced graphing\n",
    "-  [scikit-learn](https://scikit-learn.org/stable/) : provides many machine learning algorithms and tools to training and test.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RHAUKyWlWQ9L",
    "outputId": "5c09e55f-bded-4745-807c-d551d677b854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== This should generate a FutureWaring on Conversion ===== ignore this warning\n"
     ]
    }
   ],
   "source": [
    "# First, we'll import pandas and numpy, two data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We'll also import seaborn and matplot, twp Python graphing libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Import the needed sklearn libraries\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# The Keras library provides support for neural networks and deep learning\n",
    "print (\"====== This should generate a FutureWaring on Conversion ===== ignore this warning\")\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Flatten, LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# We will turn off some warns in this notebook to make it easier to read for new students\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qhZo0Q9IXUc"
   },
   "source": [
    "# Section 2: CNN for Kaggle Digit Recognition Challenge\n",
    "\n",
    "We will apply Convolutional Neural Networks (CNNs) to the Kaggle Digit Challenge.\n",
    "\n",
    "First, we will read in the digit images from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qaV6VT-66iAf"
   },
   "source": [
    "## Task 2: CNN Layers\n",
    "\n",
    "For an overview of CNNs, see [MIT 6.S191: Convolutional Neural Networks](https://youtu.be/H-HVZJ7kGI0?t=1132). While the entire video is good, the key description of CNN layers start at 19:00.\n",
    "\n",
    "We will use the following Keras pre-built layers to build our CNN.\n",
    "\n",
    "- **Conv2D**(16, (3, 3), activation='relu')\n",
    " - 16 filters, each one 3x3 pixels with default stride of 1\n",
    "- **MaxPooling2D**(pool_size=(2, 2))\n",
    " - 2x2 max pooling filter with default stride of 2\n",
    "- **Dropout**(0.25)\n",
    " - Randomly ignore 25% of the weights\n",
    "- **Flatten**()\n",
    " - Convert a 2D layer into a 1D layer\n",
    "- **Dense**(32, activation='relu')\n",
    " - Standard fully connected layer we have used before\n",
    "\n",
    "One possibly configuration would be:\n",
    "\n",
    "```\n",
    "NN = Sequential()\n",
    "NN.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "NN.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "NN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "NN.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "NN.add(Flatten())\n",
    "NN.add(Dense(32, activation='relu'))\n",
    "NN.add(Dense(output_Size, activation='softmax'))\n",
    "```\n",
    "\n",
    "Describe what changes to the CNN layers you will make. Options include:\n",
    "\n",
    "1.   Adding more or less filters in in Conv2D layer. The first parameter is the number of filters at that level.\n",
    "2.   Add more Conv2D layers. It is common to stack two to four layers together.\n",
    "3.   Consider trying larger or smaller filters. While 3x3 pixel filters are common, filters range from 1x1 to 7x7.\n",
    "4.   Try more MaxPooling2D layers\n",
    "5.   Add some DropOut layers to combat over fitting\n",
    "\n",
    "\n",
    "*Note: You should not change the input or output layers, they are fixed by our problem definition*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "colab_type": "code",
    "id": "M1H9GVNvvSdM",
    "outputId": "b3cee5e4-c07a-4f7a-e7ba-a4067b790927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model created\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 8)         224       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                73760     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 76,066\n",
      "Trainable params: 76,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set up the Neural Network\n",
    "input_Size = 32 * 32 * 3    # images are 28 x 28 pixels or 784 pixels\n",
    "output_Size = 10\n",
    "\n",
    "NN = Sequential()\n",
    "NN.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=(32,32,3)))\n",
    "NN.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "NN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#NN.add(Dropout(0.25))\n",
    "NN.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "NN.add(Flatten())\n",
    "NN.add(Dense(32, activation='relu'))\n",
    "NN.add(Dense(output_Size, activation='softmax'))\n",
    "print (\"Neural Network Model created\")\n",
    "NN.summary()\n",
    "\n",
    "# Compile neural network model\n",
    "NN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-k7Y9H-2wUr"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n",
    "                                            patience=5, \n",
    "                                            verbose=2, \n",
    "                                            factor=0.5,                                            \n",
    "                                            min_lr=0.000001)\n",
    "\n",
    "early_stops = EarlyStopping(monitor='loss', \n",
    "                            min_delta=0, \n",
    "                            patience=20, \n",
    "                            verbose=2, \n",
    "                            mode='auto')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'cis3115_MNIST.{epoch:02d}-{accuracy:.6f}.hdf5',\n",
    "                               verbose=2,\n",
    "                               save_best_only=True, \n",
    "                               save_weights_only = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Z_gd5XuZvScg",
    "outputId": "7a01959f-bfad-46e0-d9fb-dad0e6ca35ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images from:  ./CIFAR10/train_images_by_cat\n",
      "Reading validation images from:  ./CIFAR10/validate_images_by_cat\n"
     ]
    }
   ],
   "source": [
    "# Read data from the actual Kaggle download files stored in a raw file in GitHub\n",
    "github_folder = 'https://raw.githubusercontent.com/CIS3115-Machine-Learning-Scholastica/CIS3115ML-Units7and8/master/petfinder-adoption/'\n",
    "local_folder = './CIFAR10/'\n",
    "\n",
    "data_folder = local_folder\n",
    "# Uncomment the next line to switch from using the github files to the kaggle files for a submission\n",
    "#data_folder = kaggle_folder\n",
    "\n",
    "train_folder =  data_folder + 'train_images_by_cat'\n",
    "validate_folder = data_folder + 'validate_images_by_cat'\n",
    "sample_submission = pd.read_csv(data_folder + 'sampleSubmission.csv')\n",
    "\n",
    "print (\"Reading training images from: \" ,train_folder)\n",
    "print (\"Reading validation images from: \" ,validate_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45481 images belonging to 10 classes.\n",
      "Found 5612 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#batch_size = 8\n",
    "batch_size = 32\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_folder,  # this is the target directory\n",
    "        target_size=(32, 32),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validate_folder,\n",
    "        target_size=(32, 32),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LT2ZSVeXvSdo"
   },
   "source": [
    "## Train the Neural Network\n",
    "\n",
    "There are 45,481 training images and 5,612 validation testing images. \n",
    "- train_generator: The image generator above to use. Can rotate or shift images if desired\n",
    "- steps_per_epoch=1000: Number of images to process per epoch, will random choose from 45,000 images\n",
    "- epochs=100: Number of epochs. Should be large enough to train on every image mulitple times\n",
    "- learning_rate_reduction: Reduce the learning rate if loss does not keep dropping\n",
    "- early_stops: Stop if loss does not keep dropping\n",
    "- validation_generator: The image generator to use for validation, not rotations or shifting\n",
    "- validation_steps: Number of random images to use during validation, we have 5,600 to choose from \n",
    " \n",
    "### Note: This is a large data set and training may take an hour or so... \n",
    "\n",
    "This is why we are only using 10 epochs initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "colab_type": "code",
    "id": "pMeiCJoZvSd6",
    "outputId": "93a1c996-d7f1-4464-b208-8dc1b2b6d0fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 1.4222 - acc: 0.4921 - val_loss: 1.3235 - val_acc: 0.5252\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 1.3663 - acc: 0.5085 - val_loss: 1.2748 - val_acc: 0.5445\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 58s 58ms/step - loss: 1.3039 - acc: 0.5355 - val_loss: 1.2013 - val_acc: 0.5771\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 1.2700 - acc: 0.5483 - val_loss: 1.1822 - val_acc: 0.5788\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 1.2335 - acc: 0.5602 - val_loss: 1.1598 - val_acc: 0.5951\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 1.2016 - acc: 0.5723 - val_loss: 1.1585 - val_acc: 0.5992\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 1.1931 - acc: 0.5768 - val_loss: 1.1867 - val_acc: 0.6003\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 67s 67ms/step - loss: 1.1689 - acc: 0.5846 - val_loss: 1.1694 - val_acc: 0.6030\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 62s 62ms/step - loss: 1.1582 - acc: 0.5920 - val_loss: 1.1719 - val_acc: 0.6031\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 69s 69ms/step - loss: 1.1466 - acc: 0.5981 - val_loss: 1.1179 - val_acc: 0.6174\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 75s 75ms/step - loss: 1.1340 - acc: 0.5988 - val_loss: 1.0844 - val_acc: 0.6356\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 66s 66ms/step - loss: 1.1319 - acc: 0.5992 - val_loss: 1.0278 - val_acc: 0.6458\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 1.1113 - acc: 0.6084 - val_loss: 1.1031 - val_acc: 0.6224\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 1.1104 - acc: 0.6067 - val_loss: 1.0251 - val_acc: 0.6513\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 1.1047 - acc: 0.6122 - val_loss: 1.1118 - val_acc: 0.6263\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 1.1009 - acc: 0.6132 - val_loss: 1.0734 - val_acc: 0.6245\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 53s 53ms/step - loss: 1.0920 - acc: 0.6168 - val_loss: 1.0312 - val_acc: 0.6428\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 52s 52ms/step - loss: 1.0954 - acc: 0.6173 - val_loss: 1.1345 - val_acc: 0.6210\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 50s 50ms/step - loss: 1.0744 - acc: 0.6211 - val_loss: 1.0531 - val_acc: 0.6494\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 56s 56ms/step - loss: 1.0473 - acc: 0.6292 - val_loss: 1.0184 - val_acc: 0.6517\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 59s 59ms/step - loss: 1.0374 - acc: 0.6341 - val_loss: 1.1089 - val_acc: 0.6337\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 57s 57ms/step - loss: 1.0368 - acc: 0.6346 - val_loss: 1.0273 - val_acc: 0.6503\n",
      "Epoch 23/100\n",
      " 380/1000 [==========>...................] - ETA: 32s - loss: 1.0400 - acc: 0.6373"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1e09a48365e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlearning_rate_reduction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         validation_steps=200 )                  # batch_size\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'second_try.h5'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# always save your weights after training or during training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model with the images in the folders\n",
    "history = NN.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=1000,                    # Number of images to process per epoch \n",
    "        epochs=100,                              # Number of epochs\n",
    "        callbacks=[learning_rate_reduction, early_stops],\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=200 )                  # batch_size\n",
    "#NN.save_weights('cifar_weights.h5')  # always save your weights after training or during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_2GLi6avSeF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJiV__LbvSeO"
   },
   "source": [
    "## Plot the Training History\n",
    "\n",
    "We store the performance during training is a variable named 'history'. The x-axis is the training time or number of epochs.\n",
    "\n",
    "- Accuracy: Accuracy of the predictions, hopefully this is increasing to near 1.0\n",
    "- Loss: How close the output is to the desired output, this should decrease to near 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "Y965DSk5vSeJ",
    "outputId": "8b3904f7-58cf-4a29-8ce0-c12ecb99f408"
   },
   "outputs": [],
   "source": [
    "# 10. Evaluate model on test data\n",
    "print (\"Running final scoring on test data\")\n",
    "score = NN.evaluate(X_test, y_test, verbose=1)\n",
    "print (\"The accuracy for this model is \", format(score[1], \",.2f\"))\n",
    "\n",
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2,1)\n",
    "\n",
    "ax[0].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax[0].plot(history.history['val_acc'], color='r',label=\"Testing accuracy\")\n",
    "ax[0].set_title(\"Accruacy\")\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "              \n",
    "ax[1].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[1].plot(history.history['val_loss'], color='r', label=\"Testing loss\",axes =ax[1])\n",
    "ax[1].set_title(\"Loss\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAHcXCEcfzIL"
   },
   "source": [
    "## Create the Submission for Kaggle\n",
    "\n",
    "The following code generates a file named CIS3115_Submission.csv which you need to download to your local PC and then upload to [Kaggle's Digit Recognition competition](https://www.kaggle.com/c/digit-recognizer/submit).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqcMfCNsjpOJ"
   },
   "outputs": [],
   "source": [
    "predictions = NN.predict_classes(X_submit_kaggle, verbose=0)\n",
    "\n",
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)), \"Label\": predictions})\n",
    "\n",
    "submissions.to_csv(\"CIS3115_Submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5UGK9BiWDW5"
   },
   "source": [
    "## Kaggle Submission\n",
    "\n",
    "Run the code above after training the network above. It will go through the 28,000 submission images and generate an prediction for each. These are saved in a file named \"CIS3115_Submission.csv\"\n",
    "\n",
    "**Colab Users: ** The submission file is stored in the Colab files tied to this colab notebook in the Google cloud. \n",
    "1. Open the left-side menu by clicking on the > icon near the top-left\n",
    "2. Select the file tab\n",
    "3. Hit the Refresh button and the file should be displayed in the list\n",
    "4. Right-click on the file and choose \"Download\" and save it to a folder on your PC.\n",
    "\n",
    "**Juptyter Notebook Users: ** The submission file will be stored in the same folder as your Jupyter notebook file.\n",
    "\n",
    "Once you have the file, return to  the [Kaggle Digit Recognition challenge](https://www.kaggle.com/c/digit-recognizer) and select the Submit button. Follow the steps to upload your submission and see how it scores.\n",
    "\n",
    "Record your initial submission score here: _ _ _ _ _ _ _ _ _ _ _ _\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUg7_6I_ui3-"
   },
   "source": [
    "## Task 3: Report Best Score\n",
    "\n",
    "Try finding a good mix of the following:\n",
    "\n",
    "1. Number and size of convolution layers\n",
    "\n",
    "1. Number and rate of dropout layers\n",
    "\n",
    "1. Learning Rate reduction\n",
    "\n",
    "Submit your best network to the [Kaggle Digit Recognition challenge](https://www.kaggle.com/c/digit-recognizer) and compare it to your original score\n",
    "\n",
    "Base Kaggle scores here:  98.8%\n",
    "\n",
    "Best Kaggle scores here:  _ _ _ _ _ _ _ _ _ _\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUzZntKj1QFz"
   },
   "source": [
    "# Wrapping Up\n",
    "\n",
    "Remember to **share this sheet with your instructo**r and submit a link to it in Blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNmZlPcKWv12"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practice 9: Convolutional Neural Networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
